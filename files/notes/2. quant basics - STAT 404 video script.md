Module 2. Mathematical and Statistical building blocks for Machine Learning models

Module-level objectives:
1.	Identify the basic mathematical building blocks of statistical models
2.	Describe statistical distributions in the context of SM/ML
3.	Define an Algorithm


II. 
**Video II.0**: Module overview
    We're going to get more serious about the math and review some very basic definitions from Calculus, Linear Algebra, and Statistics. They are very basic, but they are also definitions that become very important when things get more complicated.
    
    However, because this is a high-level course, we will gloss over some of the details and theorems and some definitions that are not immediately necessary to us -- which doesn't mean they are not important. We will also do some calculations, but we will not drill the details, because each of these three-ish topics is essentially a full semester course. Single-Variable Calculus, Multivariable Calculus, and Linear Algebra were three separate courses I have taught. I haven't taught statistics as a standalone course, but they would likely be more than a semester.
    That's the thing about Statistical Modeling, it would require a probably a whole year full time to learn the main relevant topics, BUT you can do it knowing just enough about each thing and picking things up as you go -- with anything modern and multidisciplinary, I guess that's the current way of just-in-time learning.
    
    With respect to the math in this course, the idea is that by the end of STAT 504 you know everything that is needed and can do most actual calculations from scratch, and with that you can either learn any of the main topics in depth on your own, take further training that does go deeper into the technical details.
    
    The advantage of this approacch is not requiring this huge monolithic foundation, the quote-unquote disadvantage is havving to livev with your own ignorance about certain topics. I personally believe that this is also a good skill, because Statistical Modeling, Machine Learning, and all these AI applications are such a huge field, that even if you had an entire degree in stats, a masters in Numerical Analysis, and a Ph.D in Machine Learning, there would still be a lot you wouldn't know in detail.

    In a way this is a bit like statistical inference, you explore parameter space, find the parameters that work for you and integrate them into an aggregate that works for you. If this analogy doesn't make sense to you now, that's alright, but I hope it does by the end of the course.

**Video II.1**
+ Algorithms from computational and mathematical standpoints
    An algorithm is a series of instructions, not necessarily code.

    A digital computational algorithm, in the "20th century sense" is code, the math involved exists at the level of information theory, whereby they have to be converted into bits, that is 0s and 1s because that is all a digital computer can process.
    Discrete Mathematics is the branch of mathematics (or at least it's sometimes described as a branch, but maybe it's more like a collection of topics) that is used for many features of computation, one of them is counting the number of operations required to perform some task -- that's called computational complexity, but what it means in practice is the relationship between your input and output. I'll give an example, if you have a list of 100 numbers and want to find the largest one, you need at most to traverse the list once and record who's the winner, so that's the same order of operations as your 100 numbers, for an arbitrary number N we say that the order of the algorithm is linear with N, or is of Order N. If on the other hand you need to sort this list ir matters not only who's largest, but the exact relationship between all numbers, taht takes a lot more than N operations -- if you do it naively you'll have at least the square of N.

    That's important because some algorithms deal with a huge amount of data, so squaring a gazillion operations may mean the difference between needing a few hours to finish a task, or the remaining time until the death of our universe. However, we will see that this goes out the window when we talk about statistical models, in most interesting or useful problems there's no guarantee that you will find the best value.
    Worst than that, it's usually unknown what problem to solve, because what we want from our input is something a lot more complicated than the gratest value, or the ordering, and so on. So the way of thinking about the problems and how long it takes also changes.

    Either way, we are talking about algoriths, different kinds, but algorihtms nonetheless. In the case of data-drive algorithms it's much harder to estimate the time it takes to complete a task, it's usually based on trial and error, because you may get more data tomorrow, and this may improve your results but slow down the time it takeks to get there, or it may speed it up, or you may  decide to change your model because the new data includes features that weren't observed until now. What you can be almost 100% sure is that it will be more complicated and costly (No wonder AI companies want to build all these data centers).


+ Algebra and Pre-calculus Review
    Linear Algebra is the backbone of methods like Neural Networks, which underlie Large Language Models, Computer vision models like those in self-driving cars, text to speech, etc. They are also important in other Machine Learning methods, and some linear algebra techniques are analysis methods on their own right, like Principal Component Analysis.
    But in even more general terms, linear algebra comes up because structured data is often recorded as rows and columns of numbers, and formulating models that match these structures almost always involves making linear algebra operations one way or another.

**Video II.2**
+ Flash Review/Preview 1: Linear Algebra operations (multiplication, eigenvalues, least squares)
    - Scalars, (Column) Vectors, Row Vectors, Matrices, "Tensors"
    Even if you didn't take any college-level math courses, you're probably familiar with vectors and matrices. You can think of vectors as stacks of numbers, if it's a column-vector it'll be a vertical stack of numbers inside some tall square brackets, if it's a row-vector it will be horizontal with regular sized brackets at each side. Although it's more cumbersome to write them in the middle of text, for instance, column vectors are usually the standard, so if you see a vector symbol like a bold **vee** or a vee with an arrow on top of it, it will most likely refer to a column-vector (vectors will rarely be written out in all its entries in themiddle off text, anyway).

    In their general shape, matrices go beyond vectors and they are grids in two dimensions with numbers, "Tensors" (and I'll put them in quotes here) go further and arrange numbers in three or more dimensions.

    So when I say number here I mean a single value: 2, 5, 2/3, 16.5 10^6, and I'll also call those scalar numbers or scalars, they are a single value, they don't have different dimensions, they can only represent a magnitude of whatever. Vectors have at least two dimensions (even if the value in one dimension is zero, like [0, 2]), but they can have any number of dimensions, even if they have 3,4, 11 dimensions, they are still a single stack of scalars, they represent a coordinate in a space of so many dimensions -- anythign above 3 we cannot visualize. 
    
    - Vector Spaces (briefly)
    You can think of the _xy_ plane as a space for 2D vectors, the infinite _xyz_ volume as the space of 3D vectors and so on. These spaces don't have to be defined by the x and y, and eventually z axes specifically, but any two vectors that point in different directions define the space of 2 or 3D vectors -- that space is not extremely creatively, but very self-explanatorily called vector space.
    We're not going into the formal definitions of vector spaces, not because it's really complicated or anything, but because it at some stages of learning it can be unintuitive and counterproductive overall. For the operational purposes we need right now we just need some basic familiarity with these entitities and their home, so we're going with this kind of intuitive description.

    It's important not to confuse the number of dimensions of a vector with the shape of the vector entity itself, a vector is a single stack of numbers, it's a one-dimensional array of scalars, what changes is the number of scalar entries: a 2-vector has two entries, and 3-vector has three, and so on -- to be be crystal clear and at the risk of explaining to the point of boredom, a 2-vector is 2-by-1 array and represents coordinates in 2D space, a 3-vector is 3-by-1. Both are 1D arrays.
    `
    A matrix is a 2D array of scalars, it's a grid. It can be square or rectangular, and it is not a set of coordinates. Formally, it represents a linear operation on a vector of matching dimensions. If you have a 3D vector $v = [x,y,z]$ you need a matrix $A$ with 3 rows; it can have any number of rows, let's say it has 2, or 4, if you multiply A times v you will have a 2D vector the the first case, or a 4D vector if the matrix has 4 rows. 
    I know this can sound unintuitive as well, how can a linear operation of a 3-vector generate either a 2-vector or a 4-vector or a vector of any dimension, but we're not going there at this point. The important point is the shape of the objects and the simplest definition of what they represent: vectors represent coordinates, matrices represent linear operations. 

    Matrices are not generalizations of vectors, they are different things altogether. Tensors on the other hand are, a 3-by-3 tensor, for instance, doesn't represent a coordinate in 3D space, but it represents something like a property in this 3D space. This can be mind bending and unintuitive, but they arise naturally when we need to represent some properties like derivatives of multivariable functions. This is mostly FYI, normally what will be dicussed in Machine Learning, for instance, are "tensors" within quotes, which are just multidimensional arrays of scalars.

    So the most intuitive and generic way of thinking of these entities is as arrays, arrays are just stacks of numbers in whatever number of dimensions, a 1D array looks like a vector, a 2D array looks like a matrix (or a stack of vectors, if youn prefer), and a 3D array is a stack of matrices, and so on. If all we care about is the "physical" organization of numbers, arrays are the best description, and you can operate on arrays by just looping over the entries and doing whatever operation is necessary, but that's often not the best or most efficient approach.

    - Vector and Matrix operations and properties 
    Once we are talking about vectorization, which is employed to make calculations more efficient, parallelize them on GPUs and so on, we must keep track of what the entities are, and what operations are defined on them. You can perform many operations on "arrays", generically speaking, but some are linear algebra operations, some are generic operations. I'm not going to define every single operation in linear algebra, so what I mean by that is what you'll use throuhgout an entire first semester of linear algebra, an others that are technically possible, like adding a dimension of a vector to the last number in your car's license plate and obtaining a new vector -- you can do it, but it's not linear algebra. There are also opereations that exist, but are not those essential LA operations. So let's define the most essential operations on these objects. 
    
    - Basic vector operations: dot product, norm
    Some vector operations cannot be done because they are not well defined: for instace, you cannot add or multiply a 2-vector by a 3-vector. You could argue that the 2-vector is a 3-vector where one dimension is zero, but then which dimension would it be, the first, the last? why? This operation would be impossibly ambiguous, so it's just not allowed. You can a vector that lies in the _xy_ plane by one that has three nonzero coordinates in _xyz_, but in that case it's well defined that the first has _z=0_, and that's very different. You can multiply it by a scalar, that operations exists, and it's as you would expect, just multiply all entries by that number and get a vector of the same size with each multiplied by the scalar $s$.

    If you multiply a vector by another vector of the same size you don't get another vector. Not in linear algebra. You can of course multiply each of the entries and stick that into a new vector, that's called elementwise multiplication (you can also say Hadamard or Schur Product if you want a a=fancy name); this is used in many applications, but there are special symbols for them because they are not the most common multiplication. _The_ vector multiplication operation is called the dot product, it multiplies the entries but also sums all of them in the end, so your result is a scalar. The dot product of a vector with itself gives you the square of its size, it's length; that's basic geometry and it's easy to visualize, like the square of the hypothenuse is the the sum of the square of the other sides of the triangle, the same goes in any dimension for the hypertriangle defined by the components of a vector. We often need to compute the norm of a vector, and it can also be called the length, modulus, magnitude.

    These operations can and are often combined, you can multiply vectors by scalars, or take their do products and add the results, there will be all kinds of combinations the show up at different places. Matrix operations end up being composed of combinations of vector operations, as we'll see later.

    - Matrix operations and properties: determinant, rank
    As I mentioned, A matrix is a linear operator that transforms a vector by multiplying it, a matrix can multiply another matrix, the result is a compound linear transformation, but let's not get bogged down in the details. We won't spend time computing matrix and vector products by hand, so we need to have a general idea off how they work, but not have extensive practice on actually doing it.

    The essential mechanics of matrix multiplication are the following the internal dimension of the matrix and what it's multiplying must match, so if you have a 2-by-3 matrix multiplying a vector, it must be a 3-by-1 vector, and this internal dimension "disappears" upon multiplication and what is left is a 2-by-1 vector, a similar thing happens when multiplying two matrices, the internal dimension disappears, and the result has the remaining dimensions.

    Matrices also have many standard computable properties, but they are probably not as intuitive as the norm of a vector. One of them is the detemrinant, which I'm going to limit myself to saying is a scalar computed from the entries of the matrix and that it's needed in many operations, we'll get into its meaning later if we need to. The rank of a matrix is also a very important, it's related to the number of dimensions of the matrix, but it's not necessarily its size'. A 3x3 matrix can have rank 3, not more, but it can also be less, it will be less if for instace two of the columns are identical. I mention this because it's a very common and useful property, but again, I won't go into the details now, and we'll go there later if it is necessary.
    
    Matrices are not all created equal, they can be diagonal, meaning only the diagonal elements are nonzero, they can be symmetric around that diagonal, they can be orthogonal if the columns are all perpendicular to each other. And there are other features that can be computed from matrices that can be used in techniques like PCA and to solve least-squares estimation problems, or just to organize and do computations like those in Neural Networks.

    If you're the curious type and want to understand it down to the details, you can work through the correponding chapters from textbooks on Statistics, Machine Learning, or Linear Algebra specifically. Linear Algebra is also a core math course, so you can find a lot of videous online and full courses on edX, Coursera and [Khan Academy](https://www.khanacademy.org/math/linear-algebra) if you prefer something light. I recommend Khan Academy if you're not that keen on diving into Linear Algebra, but think a primer with some visual examples can be helpful.

    If you're not that interested and get the gist of it, you're good to go and can use whatever computational tools in Python, Matlab, Julia, Mathematica or whatever automated tool -- I don't recommend LLMs for calculations, they are not reliable, and we'll see why as we get to the statistical model fundamentals later on, even for definitions I would recommend using a textbook for it, or at least checking Wikipedia or Khan Academy. It's still important that you do really understand the gist of what we are trying to achieve with the math at each step -- minute details are not of the essences, but if you just gloss over all of it the Statistical Models will become a black box, and when looking at something like an LLM it will be like the difference between understanding the essentials of this complicated thing and trying to understand alien technology.

**Video II.3**
+ Flash Review/Preview 2: Calculus concepts (limits, derivatives, gradients)
    Calculus is the math of infinity. It allows us to do calculations that would otherwise be impossible, and in practice they are required for inference (a.k.a. parameter estimation on "training", in ML lingo), because it gives a measure of whether a change in parameters improves or not the fit to the data.
    For our purposes here, we're not going into integrals unless we reall havev to at some point -- they're harder do compute and we will probably need derivative more often in this context. In both cases, the definitions are related to limits. Limits are what allows us to calculate expresions with infinity, because instead of sticking infintiy into an expression and being stumped, instead we take a limit as that variable approaches infinity, and this seemingly simple "trick" is mathematically proven to give you a finite, instead of infinite, result; not in all cases, because the calculation can still blow up to infinity, but in many cases it will and we can used that when otherwise it would be useless.

    - Limits and derivatives
    A derivative is a slope, but it's a slope at a single point of a curve that is tangent to that curve, and it is equal to the rate of change at that point. Let's take a parabola, for instance, if we want to know what is it's rate of change at some point, we can approximate it by the slope at two nearby points, but to really get the rate of change at that point we need to compute the derivative by taking the limit as the distance between those two points go to zero, and we have the exact slope at that exact point. We're not doing all the rules of differentiation, because there are many cases, but this is often very formulaic, and quite fun really. For polynomials _x^n_ for instance it's just _nx^{n-1}_.

    Once again, we're not goint into all details and cases, what's important is that you understand the big picture and then you can move on to use automated tools as needed. For the curious, check out a textbook, there are also many open texts online, videos and courses.
    
    - Optimization
    Calculus is the mathematical basis for optimization, because it also has tools for us to determine whethe some point is a maximum or minimum, that's easy, intuitive, and visual for single-variable calculus, but almost always we will be interested in multiple dimensions. For a model with millions of parameters, billions in the case of LLMs, for instance, we want a derivative in billions of dimensions, and that is called a gradient, you may have heard about it in soe Machine Learning context; a gradient is the direction of greatest change considering all dimensions. You may have heard of gradient descent algorithms, what that means is that the method goes down a multidimensional function of the cacullatedd error, to try to minimize it and find the best possible value for the parameters -- we'll see that in greater detail when we discuss inference.

    Normally calculus is more complicated and sophisticated than linear algebra, at least in my opinion, but at this level it's also something that is needed at a conceptual level, since normally would won't have to write optimization algorithms directly to be able to model something. So again, it's good to get comfortable with the ideas, and go deeper into them as needed.

**Video II.4**
+ Flash Review/Preview 3: Statistics building blocks (probability, distribubittions, likelihood)
    Finally, statistics is the third and last of the main disciplines that serve as building blocks to Stasitstical Modeling and therefore Machine Learning methods, and therefore Artificial Intelligence applications. As the name itself says, statistical modeling is mostly statistics, and the other two are basically statistical modeling, so we're going to review it a bit more in detail.

    We're going to review some very basic definitions of probability and statisics, but they are definitions that become very important when things get more complicated.

    - Probability (very) basics: definition of event A and P(A), 
    
    An event is something that took place or can take place in the world, but like everything, may have not done so. For instance, raining, or you waking up late. Even raining in the desert is not an impossible event, maybe even less so nowadays; even the sun rising tomorrow is not a complete certaint - we expect it to do so "forever", and it probably will for millions billions of years, but physics predict that the universe will collapse one day, so there's a chance the sun won't come up, even if it's a long time in the future when someone asks that question -- I'm glad I won't be there, I'm not sure I could live with that kind an anxiety, but that's problem for psychology, not statistics. 
    
    Events have probability of happening or not happening, so if you believe something is a random event, you can define the probability of it happening a P(A), the probability of an event A, if you have two events you can define the probability of each happening and both happening. The probability of it raining and you waking up late is P(A ^ B).
    If this is not familiar to you, this is something that we can visualize better with practical examples with objects, for instance, the probabilty of getting a red marble out of a box with 2 red and 3 white balls and of a different box with 2 purple and 6 white marbles. The probability of sticking your hand in the box and getting a red object is 2/5, 40%, or 0.4; on the second box, the chance of finding a purple one is 2/8, 25%, or 0.25.

    If these things are totally independent, and in the case of separate boxes they are, P(A ^ B) is equal to P(A) times P(B), so the chance of picking up a red and a purple marble is .4*.25 = 0.1, and of picking two white ones is .6*.75 = .45 (the remaining possibilities are picking out one white and the other of one of the colors).
    If these events are somehow not independent, those calculations don't hold, and we must find a way of formalizing that correctly. For instance, if depending on the color of the first ball you are given a different box, you have to take that into account


    - conditional probability joint (intersection A and B) probability, union probabilty (A or B), 
    Enter conditional probabilities, P(B|A), meaning the probabilty of B happening given that A happened. Let's continue with that example so we can build the intuition before making definitions. Supposing that if you choose white on the first box ou are given the purple box, but otherwise you get a yellow box with two yellow and four white marbles. Now we want to know the chance of picking a white marble in the case that you picked white on the first, or red on the first, because it will be different. 
    This is conditional probabilty, let's call picking red $R$ and white $W$ in the first box, in the purple box let's call it $P$ for purple and $W$ for white, and in the yellow one $Y$ and $W$. P(W)=0.75 as we saw before, but P(W)=2/3 or 0.66, so our probability is conditioned on what happened before, we write that as P(W|W) = 0.75 and P(W|R) = 0.66
    
    
    The definition of conditional probability is P(B|A) =  P(A ^ B)/P(A), the probability of both happening divided by the probability of the conditioned on having happened. In our example if you divide P(W^W)=.45 which we computed before by P(W) in the first box, we get 0.75, if we divide it by P(R), we get 0.66. 
    
    These visual examples can be useful to get an intuition and understand, and you probably understand those because they're fairly simple. However, it's important to learn the definition and using it as an  axiom that needs to be applied not questions asked. So I suggest doing two separate things: making sure your understand the intuition on this simple example, because then you know what you're talking about, but that you also accept the definition of conditional probability as a given, and work with that definition when you need to do the calculations, because it won't always be intuitive to break down the real world problem into understandable and relatable terms to the probability axioms.
     
    Sometimes it gets more complicated, sometimes you have continuous probabilities, and you won't always be able to make that intuitive connection. I know it's kind of weird having like two separate things and not being able to always connect them perfectly, but probability is sometimes a difficult thing for the mind,a s Daniel Kahnemann said, it's a system we have to grapple with, not something that comes naturally. So it's it's useful to sometimes, as a physicist would say, just shut up and do the math.
    Nevertheless, that doesn't mean probabiltiy will always be a mystery. As you go on, apply probability and statistics you're going to build intuition about how that works, and you'll be able to identify the relatoinships and intuitively grasp what you're working with, without having to stop and try to understand and then move to the definition.

    But once again. Our objective here is to get the practical matters in formation, and have the definitions so we can work with and apply it later in the statistical modeling settings. It would of course better to understand things in at a deeper level, but that requires several courses in probability several semesters, and we're not going to do that here. 
    I encourage anyone who is interested to look up courses and textbooks on statistics and probability, but again, that is not required here. There are courses in Khan Academy, there are books, there are videos, you can always check them out, that deep level of underessential to our machine learning applications. 
    
    In my opinion, statistics and probability, probability especially are a lot less intuitive than math; some concepts in math are kind of alien, like Infinity, which is unlike anynthing we deal with daily, but once you get down to the mechanics you just apply those and it's easier to understand. Probability has an, I wouldn't say subjective, but an interpretation element to it that you really need to get right,, and it's at the core of modeling, and it takes a lot of practice to really have that intuition. That's why I find it harder, but that's kind of how it is. We have to work with what we have and will have a solid degree in statistics and then move on to do modeling. I don't have one, in fact, I had some formal training statistics and probability just like I had in math, but my formal training is in the natural sciences. So my knowledge of math and statistics is a product of years, decades now, of working with that kind of formalisms and applying them and learning as I go. And I think that's. I wouldn't say better, but a more sane way of doing it if you're interested in the real world applications and problems or research problems, as opposed to the math and statistics by themselves.

    - probability distributions: Discrete
    Once we have a set of events at their associated probabilities, the next thing we will probably want to specify is a probability distribution, which means just that, how the probabilities are distributed among the avavilable outcomes. In the case of our boxes the probabilities are distributed between red and white in the first box, and white and the other color in each of the boxes. Other common distributions are heads or tails, for instance, a 50-50 chance, or 1 through 6 for a 6-sided die, which equal probabiltiy, 1/6 for each.
    A visual way of writing discrete probability distributions, and distributions in general is onn the xy plane, where the event types or values are on x, and the probabilities on y. We can write two colors, heads and tails, 1 through 6, and a dot or marker at the probability value, but commonly will have a spike under it.
    You can set up distributions of any events, but commomnmly there are parametric distributions that are used for many different models, these are distibutions like Poisson, Geometric, Binomial, Negative Binomial. Each of those arises from a kind of process, but it can be used in different problems. 
    
    The geometric distribution, for instance, is what you get for the number of tries with a certain probability until you get an event -- if you have a one in a million chance of winning in a Casino, you may think that it's worth playing a 10 thousand times and you'll have a decent chance of winning. Geometric distribution with a probability p=1e-6 will tell you it would on average take a million tries to win; house always wins, so the probability and cost of playing will certainly be adjusted so that on average it will make a lot more money. Nothing will change that, but at least with statistics you'll know where not to spend your money.

    The binomial distribution is the the distributions of the number of times you'll win if you play a certain number of times $N$ with some probability $p$. So it frames a similar problem, but framed in a different way, the values are still 1,2,3... so on, but the geometric can to up to inifinty and in the binomial there's a max number of tries, or trials, in statistical language.

    - Continuous probability distributions
    We discussed discrete probability distributions, that means we can count the individual events, or they are integer numbers (which is called countable infinite). Another kind of distribution is a continuous distribution, like what you see with the height or weight of people -- there's nothing saying they need to be either 1 or 2 units different, or half or a third, the difference can be infinitely small, and for that we can use a distribution that can take any existing value, like the gaussia a.k.a. normall distribution.
    In the case of continuous distributions we can no longer talk about the probability of a event, or probability mass, because there's no discrete event, the value is no longer 0,1,2,3, for instance, there are infinite options between any of those numbers, so we instead talk about probability density.
    In an applied setting, the distribution we end up using depends on the kind of data we haev, as well as the problem, it's a modeling choice, but it's a very important one, it cannot be arbitrary.

    - Properties of distributions: PDF, CDF, mean (median and mode), variance, moments, expectations
    Parametric distributions are functions of its parameters, and they havev properties. Some of them are summaries like the mean, which is also known as the expected value, and the variance, which is related to the deviation around that mean. There are other summaries like the median, which is the midpoint in the distribution, where you have 50% to one side and 50% to the other, and the mode, which is the highest value, the peak of the distribution.
    There are many things you can compute with the probability distribution, those are called expectations, the mean is only one kind of expected value, which is the expectation of the variable itself, but you can compute the expectation of its square, or any function of it you like -- the variance, for instance, is the expectation of the square of the variable minus the mean. 


**Video II.4**
    - "likelihood": 
    likelihood is sometimes used as a synonym of probability, but we'll see that likelihood has a slightly different meaning when we are talking about models and the likelihood of a set of parameters, so I just want to issue a word of caution when using this term, to be precise about what you mean. You can absolutely talk about the likelihood of an event, and you can use it more loosely to sayn something is likely or unlikely; HOWEVER, if you're talking about models, it's a good idea to be precise about the terminology, and make sure you can formalize what you are saying in plain language. The definition and use of "the Likelihood" in the context of statistical models is a common source of confusion; we're going to discuss that in modules 4 and 5, so for now this is really just a heads up on that specific term.
 