# Statistical Modeling (SM) I: Simulating random outcomes


## What makes a statistical model different from a mathematical one: Deterministic versus Probabilistic

- The mathematical models we described so far are Deterministic Models, that is, give their explicit or implicit formulations we know exactly what value it outputs for a given input; therefore, we can compute their trajectory exactly anywhere in time, space, or any dimensions accounted for in the model.

This holds for implicitly-defined models and their approximate solution; although there is a difference between that and the exact solution (and it being called an "error"), the output of any given method will be always be the same, there is no randomness involved in this solution.

Models that are not deterministic are called Probabilistic and maybe more often Stochastic, which is just a fancy work for random. The natural world is built on randomness (or stochasticity), but computers are only able to produce deterministic outputs (with very few exceptions);  modeling real systems, therefore, requires randomness to be modeled. To produce seemingly random numbers computers rely on Pseudorandom Number Generator (PRNG)Links to an external site. that pass for the real thing for many (but not all) applications.

Given a PRNG - which are built in to any programming language and often implemented for several common statistical distributions - one way to produce a stochastic outcome is to add noise to a computed solution. We can for instance draw (pseudo)random numbers from a normal distribution with mean zero and some variance and add to our exponential curve; the result is a stochastic version of our population model. Another approach is to add noise at each step of the numerical solution - the difference is in the former case the deviation from the deterministic solution is independent at each point (because it is computed first without noise) and in the latter it propagates along the trajectory (noise added to a computed time point influences the computation of the next time point), i.e. noise compounds along time. A third (and famous) method includes stochasticity in the times when events happen (as opposed to an equally-spaced discretization of time), this is called the Gillespie Algorithm.

Given that our outcome is no longer deterministic in this latter case we no longer talk about an output y = f(t,\theta), i.e. a deterministic function of a dependent variable and additional parameters, but about probabilities of multiple trajectories 
Y \sim P(t, \theta). The expression denotes that the left hand side is a random variable distributed as the probability density on the right hand side; more commonly we talk about p(D | \theta), the probability of the data given the parameters.

As an example let us assume our model is linear with the form y = ax + b for the mean output, symmetrically distributed as a gaussian distribution with variance sigma^2. Putting some numerical values to the parameters, say a=1, b=2, \sigma=1, at the point where x=2, for instance (we can compute that for any and all other points on x we'd like), we have y = 2\cdot 1+2 = 4. Our statistical model is that of a normal distribution around that mean, so the probability density at that point is given by the normal distribution \mathcal{N}(\mu=y(2)=4 | \sigma=1). If there is a data point Yi=5, we can compute its likelihood - the most likely outcome in a normal distribution is its mean, but we also expect nearly 70% of the observations to be within one standard deviation of the mean, and 95% within two SDsLinks to an external site.. Here \displaystyle p(D | \theta) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2}\frac{|Y_i-\mu|^2}{\sigma^2}} = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}} \approx 0.24 
(for comparison it would be close to 0.4 if Yi = 4 and drop to to around 0.05 if Yi=6) where the parameters are summarized as \theta = [a, b, \sigma], and data D = Yi is computed for one data point. The likelihood of the model would be the product of the probabilities of all data points.

So here the probabilistic model would be just the straight line given by the function y = f(x), to compute any values there we would have to have to choose some values for a dn b and it would always generate a straight line for this set of parameters, while the probabilistic model would require us to specifiy (in this case), and additional layer given by the normal distribution of probability, so we need to specify an additional value sigma for the distribution standard deviation, and assume this deviation is around the value of the deterministic function.


- Sampling distributions, Confidence Intervals, and Statistical Tests

I woudn't say this is the canonical way of teaching statistical modeling, but I think it's a practical approach, and we're trying to explain things in a conceptual rather than a very formal way.
Humans often find Probability and Statistics to be deeply unintuitive subjects, so it is not surprising that mathematics is several thousand years old (with even some animal species apparently being capable of some arithmetic) while probability and statistics only being developed two to four hundred years ago. Unfortunately, this fact is rarely acknowledged by statisticians and statistics instructors who teach the essential concepts outside of a useful context and (here this is a personal opinion) do a disservice to students and to researchers of other fields that have to fit entire research programs into a narrow view of statistics.

Here I will first briefly introduce some basic concepts of statistics and how they fit into research outside of statistics itself, I'll tend to use examples in the life sciences, and then flip everything on its head to try and present a more useful approach.

    + Sampling distribution: it is a distribution obtained from a sample of some statistic, e.g the mean number of RNA counts for a gene.

    + Confidence Interval: is the range of estimates for a parameter at some level, e.g. "given some expression data, we estimate with 95% confidence that the mean expression is between 100 and 500 counts for gene X".

    + Statistical Test: is a method to decide whether one hypothesis or another (e.g. a null hypothesis) can be rejected in favor of of its alternative.

Those are all the definitions you need to learn to perform a statistical analysis, and yet it can be extremely confusing and sometimes vague. Common questions can be: "if the observations come from a distribution, why bother with a sampling distribution of the mean instead of trying to estimate the original distribution in the first place?", "Does the confidence interval mean that I am 95% sure we found the true value?", "What is a hypothesis?", "What is a test doing exactly?".

While it is good that statistics is used in research (as opposed to eyeballing the data and going with your gut feeling), what usually ends up happening is experiments and entire research programs are developed around data formats that can be shoehorned into statistical frameworks. For that reason it seems that model-based inference is an entirely different approach, but traditional approaches are also based on models, and also rely on inference, just it is all hidden and used without the researcher thinking about it, nor controlling it.

Since we have been discussing models for a few sections now and have begun discussing probability and statistics we can go backwards and try to describe the concepts above in terms we have been discussing. A hypothesis is a model, often a simplistic one (because the experimental design may only assay two conditions) like estimating two means 
(which are simply parameters); since there is always uncertainty associated to any estimate there are (sampling) distributions around each parameter; finally, their 95% probability ranges are a measure of how similar the parameters are.

In sum, a statistical test is a model comparison based on distributions estimated for specific parameters. So learning how to do model-based inference is actually a general approach to statistical analysis.

## Formulating stochastic models
Stochastic is just a fancy word for random or aleatory (which I guess is also fancy, since it comes from the latin for rolling the dice). Fancy or not, I think it's actually a nice word, and to me it conveys the idea of "dependent on probability", while random and aleatory sometimes sound like there's nothing but dumb chance to it. 

It is also used slightly different than probabilistic: a probabilistic model, for instance, may be that associates probabilities and probability distributions to certain outcomes, as we'll see in the next module on inference, while stochastic model usually denotes a model that will generate a single draw from a random process. You may have heard of "Stochastic Parrots" in association with Large Language Models. That's a derrogatory way of saying they are spitting out words that are related to what they "heard" (i.e. what was prompted), but are otherwise random draws from a statistical model -- it's actually not a bad description of LLMs, but we'll get to them in due time.

To be clear: aleatory, probabilistic, random, stochastic are essentially synonyms, but you will see them used more or less in conjunction with certain other terms and in certain contexts, so it's worth knowing which is which. In this video, our focus is on simulation of probabilistic outcomes, and for that "stochastic model" is probably the most common term.

So, how do we build a stochastic model? There are many ways, like starting to think about probabilistic processes from scratch and putting together a model from the result of these processes, but I think a more straightforward way is thinking about a deterministic "skeleton" and going from there. Using some curve defined by a function we then add a probabilistic layer to it, simialr to what we illustrated with the linear model

- Pseudo-random numbers over a deterministic solution as generative process

how does this go? In our linear model example we computed the value of the function, and looked at the probability of observing some data value around that value. Here, the structure is the same, the only difference is that instead of looking the probability of a fixed data point based on the distribution we chose for the model, we are going to simulate a random number from this distribution. 

So for our example where the computed function value was 4, and we assumed the function would give us the mean of the distribution, and we also had a standard deviation, we draw a random value from this distribution, and that's our simulated value. Easy right? Kind of, you may have questions about how do we acctually draw random numbers, and the easy answer is there are tools for that, but there's a more complicated answer. Before we go there, let's just clarify some more terminology that is common when going through this process:

    + the value we computed from the function for a specific set of parameters is often referred to as a "prediction", that's mostly because we often assume that it's the mean value and maybe the most likely value (in the case of a distribution like the gaussian or normal distribution, that's true), but also because in the context of inference we don't know the true value of the paramters, so we rely on making these sort of "predictions" for parameter values that may or may not make the model fit the data
    + The value we actually draw from the distribution is called a realization, that's because until you actually draw the value, we don't know what it is, we only know the probability with which they can happen.

So back to random draws, as a mentioned, for every question there's an answer that is clear, simple, and wrong. In this case the answer was that there are tools that do that. You can pick up any language you like, probably easier if it's one that just drops a shell you can type in like Python or Julia, and you import a library like Python's random and use the `random()` function that will give you a random* floating point number between zero and one(but since I'm giving a proper explanation here I'll but a big asterisk on random), or `randint()`, which gives you an integer number between the numbers you choose, like `randint(0,10)` which will give you a number between 0 and 10. Both give you a number with uniform probability, all numbers have the same chance of coming up, like when rolling dice, that's a uniform probability between 1 and 6, and `randint(1,6)` is a computational implementation of that.

The reason this is wrong is because computers, at least the computers we use, are deterministic machines, they can only do precise calculations we instruct them to using 0s and 1s, so they are incapable of producing stochastic outputs. What were are using here instead are pseudorandom numbers, and that's what programming languages have implemented: pseudonumber generators based on algorithms like Mersenne Twister. We're not going into the details of these algorithms, for our purpose we should know these are not truly random numbers, they cannot be used as such in cryptography and some more sensitive applications, but for many things they are enough -- to simulate draws from a normal distribution in this course they are definitely enough.

So in addition to functions like random() and randint(), scientific computing libraries like Scipy in Python or built-in Julia libraries, but really nearly any programming language one way or another, will implement draws from the most common distributions. There's a way of converting the random number between 0 and 1 into a value in any distribution, because a probability distribution integrates to one, the total probability of all values must integrate or sum to one, so you can write a function that computes what cumulative probability corresponds to your unuiform random draw, the actual algorithm is not essential to know here.

In a nutshell, random() will pick a ranodm number between 0 and 1 with unifrom probability, all numbers have the same probability; in the normal distribution you get the mean or numbers close to the mean with higher probability and far away values with lower probabiltiy, just as you see in the graph for the distribution, and in any other distribution it will pick values according to the probabilities in its PDF -- as you may know from other courses and from our crash course, flash review of one or more semesters in statistics from previous modules. We're going to see more about hwo to choose between different kinds of distributions in the next module, for now we are interested in how to generate one or more realizations from disributions with an underlying mathematical model.

Let's put it all together in one place. Let's use a different model for a change: our logistic growth model. Given the model parameters we can compute a logistic curve, that's the deterministic skeleton and our prediction for any given point in the range of $t$. Our next step is to choose a distribution, that is problem dependent: if we are are modeling the increase of a disease that eventually infects all hosts, we can use the normal distribution if we are modeling the prevalence of the disease, for that we have to specify the standar deviation parameter, and we draw pseudorandom numbers at any points we like in the range. With the same model we ccan have a different distribution entirely, logistic curves are often used in binary classification problems, in that case we would have a Bernoulli distribution, which is a very simple distribution that is equal to either zero or one with some probability $p$, if this is what we are doing here, the logistic function would have to be between zero and one give us $p$, and the draws would give us zero or one at each point: almost certainly zero on the left side, almost certainly one on the right side, close to 50-50 as either side approaches the midpoint of the symmetric curve.


- Stochastic processes and observation noise: simulation of a generative process

In many cases we can compute the entire function at once, but in other cases we may need to, or want to compute them by parts. Staing with our logistic function for a bit, for classification purposes this will always be computed as a single function all at once, because there's no point in doing it otherwise. But if we're thinking about it as a growth curve, we can see it as a sequential process: the extant individuals at time `t=0` reproduce and generate individuals in the next generation -- in a continuous deterministic process this will generate the exact same curve as the analytical solution, but if we want something closer to the real world, where populations and reproduction are not continuous, there are discrete individuals and couples, one or two, not 1.3 or 1.7 organisms, and reproduction happens often bewtwee two individuals at some point in time giving rise to a later generation. In that case we can thing of these individual events as stochastic, so for an initial population of `N` individuals with growth rate `theta` we would need to make a modeling decision on the distribution of the number of individuals at that later time point.

Parenthesis here, everything in both the mathematical and statistical parts of modeling is a modeling decision; there's nothing saying you have to use a specific function for a specific system or process. That said, there are things that make more or less sense, things that make a lot of sense and others that will make no sense at all. But nothing's forbidden.
I cannot teach all possible choices, and they're that, choices, so you'lll have to think about them -- that's the kind of things that will never be automated by AI, beucase there may be such a thing like a stupid idea that nobody ever tried, but that all of the sudden makes sense in some obscure or no longer obscure context.

But going back to our growth function, here we're particularly interested in out differential equation form, or in the case of discrete time, difference equation -- it's the same idea, but in discrete time intead of continuous. If at time zero we have `N_0` individuals, at time 0+t we can have on average `N_0` times theta times Delta t (delta t here is t minus 0) minus the quantity in the second term of the diffential equation: kappa times `N_0` squared times delta-t, kappa has to be much smaller than theta so that if the population is small population will grow nearly exponentially and will slow down as it gets asymptotically close to carrying capacity, so for simplicity here let's approximate this early growth as N0-theta-Deltat.
If we assume this is the mean, we could pick the normal distribution and specify a standar deviation, or we can pick a discrete distribution like Poisson that has the variance equal to the mean, and draw a random number for the next time.

The next time point, we repeat the procedure, compute the growth from the values at the previous step, use it as the mean of the poisson and draw a number. You can see that this is different from just computing the who curve and drawing PRNG from each point; using this approach this is a lot more costly, and requires you to compute each step before the next, it will also cause larger deviations from expectation, because by chance you may get extinction of the population if they are "unlucky" and have a very small growth many times in a row. In the previous case they will never be extinct, even if you have a small population at some point and draw a zero value for that time point, the later time points are not zero, so you may draw a positive number from them, that's quite artificial.

This kind of modeling is called discrete-time stochastic model, and you will usually pick a fixed interval, like a day, and do that procedure for each time point for some interval, 100 days, a year, two years, whatever you'd like to simulate. An anternative to this approach is the Gillespie algorithm, where we draw a random number for the time for the next individual event, and then increase or decrease the population by one. That's even more constly because each individual birth or death is at least one random number (in the previous case it would be several births per draw), but it can be more realistic in some cases.

So, in sum, we've seen two ways of converting a deterministic model into a probabilistic version of it. Depending on your previous quantitative background, you may be a bit overwhelmed by the number of different concepts and choices involved, but that's not unexpected and doesn't stop you moving forward. You should remember understanding these in depth requires at least 1-2 semesters in Calculus and another 1-2 in Probability in Statistics. The goal here is not to understand all the nitty-gritty mathematical detailsso, but understand the overall process, because there's still more coming out way. The good news is that they are all based on this general ideal of probabilistic models, so try to spend some time internalizing some of the main points here, and this is also a good time for you to ask me questions about this, either via forum or in the live office hours.

## Parametric vs nonparametric models. 
Before we move on to the next module, Statistical Inference, I want to consider another important dinstinction in terms of kinds of models, and that's Parametric versus Nonparametric models. Nonparametric models is king of a misnomber, because all models have parameters, so some people propose saying "model-free"; that's probably a misnomer as well, but maybe less so. Maybe that's the kind of thing that doesn't have a good name, like low-calory pizza or hassle-free support customer support, and instead require a few sentence to get to the point.

What nonparameric model is intended to mean is that it is not constrained in the same way as parametric models are. Our logistic equation may be a very nice and useful models, but if the data is not a monotonically increasing function, it will be hopeless to describe it using logistic growth. Sometimes we cannot test all possible options, we may look at some data and try to figure out if the pattern is linear, qudratic, logistic, exponential, decreasing exponential, nth-order polynomial (which is a quite dangerous thing to use in most cases). Instead, there are models that don't have a particular shape, like

    - B-Splines, which are a piece-wise combination off polynomials, 
    - Gaussian Processes, which are Multivariate Gaussian distributions that describe the correlation between observations
    - Neural Networks, which are a sequence of layers of linear models with activation functions and a variable "architecture", which we'll get into a bit latter on
    - And many others

Non parametric models are great when the pattern is not clear from the data, or when you don't know or cannot know much about the process that generated the data, or if you have a lot of data and need something extremely flexible that can fit to it and capture some hidden patterns in it. Or all of the above. We'll see in a couple of modules that nonparametric models are the workhorses of Machine Learning, or ML, which in turn is the engine under essentially everything that is called AI today.

Now. In an ideal world, I think we should strive to model things with parametric models, because we should want to understand the systems that we are working with. That is especially true in scientific research, where we are interested in mechanisms. It doesn't mean researchers cannot use nonparametric models, ML, etc, but parametric models in addition to being more powerful, because through your modeling choices they make a strong statement about what you believe to be how the system works, they are also more precise statistically, in part for the same reason -- if you're making strong assumptions and you are right, the model will fit well, if you are wrong, it won't fit. If you choose a nonparametric model that will fit anything, it will say little to nothing about the system.

So here's another strong statement about AI and Machine Learning: AI/ML will not and cannot replace parametric models, so anyone saying you should stop using differential equations (or whatever mathematical model is used in some field) to model some process and instead through a Neural Network at it, most likely doesn't know what they are talking about. So I don't come across as an AI-skeptic, or a hardliner, or whatever, that is NOT to say that:
    - there aren't cases where you can use ML instead of parametric models, but that's usually when the parametric model is functionining as a generic model to describe the data, not a representation of mechanisms (biological mechanisms, physical mechanisms, quantum mechanisms, etc...)
    - AI/ML can't serve any purpose in finding mechanisms. They can, for instance, you can find a pattern nonparametrically and then try to come up with parametric models that are compatible with that pattern.
    - it's not to say an LLM cannot assist you with parametric modeling, it may work as a copilot there, but in my opinion it cannot replace original and creative research. That's a different and more philosophical discussion which is probably beyond the scope of this course.
    - it also doesn't mean that if it's impossible to reproduce what is now being called General Intelligence, or Superintelligence, and that would solve a lot of real problems, but I also doubt that is remotely close from the existing tech, and if it does happen, we have bigger problems than a philosophical discussion about research into natural mechanisms.