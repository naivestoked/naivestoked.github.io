# Statistical Modeling (SM) II: Inference

## Inverse problems: finding the culprit without witnessing the crime
        
So far most of what we did was to put together mathematical constructs that allow us go generate some output. It foes a little bit like: choosing a kind of model (of course based on how it's realted to the problem we're trying to solve), somewhat arbitrarily choosing the model parameters, and computing the model output. 

That's a valid approach, this was how Imperial College researchers estimated the number of COVID infections if no restrictions were implemented -- maybe it's a controversial example, because that simulation may not have been very accurate, but that's exactly the point. If you don't have sufficient data to compare to your model output, you may be just making a wild guess. 

In the case of forecasting that is alway the case, because there was no data at all, but as the pandemic progressed, you could compare the real epidemic data to the model prediction.
I'd say it's very, very difficult to predict that, and most complex phenomena in that way. The inverse approach is to choose or develop a model that is suitable infer the parameters that best fit the given data, that's Statistical Inference.


Let's step back a second to define inference in simple dictionary terms before getting into its technical, statistical meaninng. From the Oxford English Dictionary:

_The action or process of inferring;_
_the drawing of a conclusion from known or assumed facts or statements;_
_esp. in Logic, the forming of a conclusion from data or premisses, either by inductive or deductive methods;_
_reasoning from something known or assumed to something else which follows from it;_

yeah, sorry for the boring list of definitions, but in general terms this mean that inference is like drawing a conclusion from known facts or data. It's like what's called deductiton in Sherlock Holmes, although technically deduction would be an exact conclusion, like a theorem that follows logically and unequivocally from the premises or axioms. What Sherlock is doing can be really considered inference more than deduction:

- there's a crime: in Aurthur Connan Doyle's novels usually someone is dead, or something very valuable was stolen
- there's evidence: someone in a dark coat was seen, maybe there's a muder weapon, blood stains
- there are motives: people who were mortal enemies with the victim or  who'd benefit from their death

In a police investigation the existing evidence also guides further investigation, until there's enough to make a complete picture that is sufficient to charge and convict a criminal under the criteria of the land's court system.
The easiest way to convict a murdered is if someone witnessed and recorded the crime -- and even then the witness must be credible and the recording verified to be authentic. Nothing can be considered universally true with 100% confidence for philosophical reasons even, our sense are fallible, they are affected by our emotions, mental state, so a full picture needs to be constructed to allow for that uncertainty to exist, while at the same time giving us enough confidence and credibility on the conclusion we reached.

Statistical inference in particular goes along those same lines. It's actually comparatively simpler, because there are strict, objective, quantitative criteria. There are again, however, somewhat subjective modeling choices which can make the conclusions more or less credible. We're going to see in detail the process of using statistical models to solve inverse problems


## Inverse problems in science, mathematics, and statistics
Beyond inverse problems in everyday life, this kind of problem is among the most complicated in science, math, and statistics as well. In science, data is often gathered or experimentally generated to try and understand some phenomenon, but we often only observe some output from the system, not the cause -- for instance, you could possibly see in-vivo with a microscope or some other probe that a specific enzyme attaches to a protein and the protein goes on to have some specific activity (again, correlation is not causality, but some things are fairly obvious in some systems), or you could observe that when the amount of X is increased the activity Y goes up, and infer that X causes Y. You could theoretically Sherlock-Holmes your way to a scientific conclusion, but the most correct approach is to assess that statistically, that and everything else that requires data.

In mathematics, which is still a more exact and abstract context with no real data, inverse problems are still much harder than their forward counterpars. Even in simple examples we can see that: 
- A matrix-vector multiplication is a simple operation as we've seen: multiply each row by the vector, computer the dot product, and stick the result into the corresponding entry in the result vector. The inverse problem of finding the solution to the equation Ax = b when x is unknown requires moving things around, eliminating variables in different rows until you can determine their values one by one, or equivalently it requires inverting the matrix, which is much more complicated, or some other more efficient method, but still never as sttraightforward as simply computing a series of dot products.
- Integration, in Calculus, is another example. Integrals are the inverse of differentiation, and finding derivatives is a lot easier than finding integrals. You can always find a derivative, you just need to work through it and apply the different techniques, but it's not always possible to find an integral, and even when it is, it's often a cumbersome sequence of trying different techniques in the hopes of reverse engineering the differentiation techniques that tooks us there, reaching a dead end and trying something else, or repeatedly initegrating by parts not knowing if that's going to work.
- There are other more complex inverse problems in mathematics, but the essence of the difficulty is their invtheerse nature: instead of having to perform complicated or uncomplicated operations, you have to think, what are the operations that could've brought us to this result in less time than that remaining in the known universe.

Finally, there's statistical inference. It's impossible to know from the data alone what model generated the data, and what were its parameters, but given a model of choice, there are methods to systematically estimate the parameter values in that model. Let's start simple and use a known model, let's go with our simple linear model with slope  parameter a=1, and intercept parameter b=2, we can plot the output and mark the points every half unit, exactly on the line as they have to -- this is a mathematical model, it's deterministic. We then choose a normal distribution with standard deviation sigma=1 and draw a random number from the distribution at each point. So far we're only simulating a stochastic outcome, in real and more interesting situations we wouln'd have access to this steps, we'd just see the data, and maybe we think it's ok to just use a linear model for it.

Now let's imagine the reverse order of things: we're presented with the noisy data, another name for stochastic realization, we don't know what the model that generated it was, and much less the parameters, but we can kind of figure out that there's a linear trend so we're going with a linear model. We don't know what the statistical distribution is either, but we assume a normal distribution. Right there we made two huge modeling choices and assumptions, we assumed that the trend is linear and that the distribution of the data is normal around that line -- we happen to know this is true in this case, but we normally don't, and it's normally not that clear. We're gonna gloss over this "small" detail, and move forward, what we want, is to be able to compute a measure of fit, and we can, but almost always it's by trial and error: we pick a value, let's say a=0.5 and b=3, and sigma=2, and we compute the probability of each data point given the normal distribution, then we pick a=1, b=3, sigma=1.5, so the same thing, compare. We have a winner between the two, but what about the other infinite number of parameter combinations? That's where inference algorithms come in, we're going to flesh out the algorithms in the next section

## Inference: learning about the model from the data
- Inference algorithms: analytical, optimization, sampling
We've gone forwards, we've gone backwards in formulating our probabilistic models, we've seen how we can compare the probability of a set of data points objectively, now we need a way of finding the best parameter set (or sets) out of every possible combination of parameters.

The first component to doing that is understanding what we're doing, in our linear model example one way of thinking of is minimizing the "distance", here distance's within quotes, because there are different ways of specifying that. You may have heards of Least Squares, or minimizing the squares, that means that we want to make the squared distances from the model to the data the minimum possible. In very, very specific cases there's an analytical solution to that problem that can be found by simple linear algebra formulas, that will give you the one set of parameters that gives you the least squared error between data and model. 

Another measure of model fit is "The Likelihood", capitalized "The" and all we mentioned that in the statistical building blocks section. The Likelihood is a different and more general quantity to assess model fit

For all other cases, which is essentially always, an optimization algorithm is needed -- an optimization algorithm can minimize or maximize a function, and that's what we end up having here, if we write the sum of squares given some data, that's also a function, and we can use optimization to find the minimum. Getting to this function is a bit more involved, but we'll get into it in preparation for this module's assignment.

Another approach to inference that is really, really powerful, but usually comes across as odd when first presented is sampling. Sampling is trial and error, just pick a value for the parameter sets, compute the measure

- Frequentist and Bayesian Inference: modern methods for "curve-fitting"
- Model selection: Overfitting, regularization, and Occam's razor

