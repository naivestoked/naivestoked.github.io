# Machine Learning (ML): Statistical Modeling meets Big Data

## Philosophical differences between SM and ML
- I was going to start this section with "I don't want to be pedantic", which is what everyone who's about to be pedantic says, so let's try something else. To begin with, Statistical Modeling is not the same as Statistics, it's quite simply a process of using math and statistics to model or emulate soem real-world phenomenon. I wouldn't compare Statistics to Machine Learning, I'd compare Statistical Modeling, but again let's try not to fail in being unpedantic and use them interchangeably for a minute here.

Since we didn't start at the beginning, let finally get to the beginning without further ado: what's the difference between Statistics and Machine Learning? A Nature Methods paper from 2018 with the title "Statistics versus machine learning" tries to answer it in the subtitle with: "Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns."

What does that actually in mathematical terms? Nothing, really, it doesn't mean anything in terms of the concepts we discussed, this is totally a difference in practice. We saw what the first part of the quote means "drawing population inferences from a sample", that's exactly performing statistical inference (the word we used less maybe was population, but it's implied that when you perform inference from a data set, you're trying to learn something about the system that generated it. I atually prefer system rather than population, because it gives an idea of a fixex group of individuals, but that's a personal take on the nomenclature).

About the second part: "ML finds generalizable predictive patterns", I know what it means, but otherwise I don't think I'd understand the sentence on its own merit. Statisitical Modeling also attempts to find generalizable patterns, but they keyword here is predictive. If you have a very good model of your system and good data observed from it, once you are able to perform inference it will be able to make general statements about the system and you'll be able to predict what would happen in the future, or extrapolate the range of observations beyond the original one. Here lies the core of Machine Learning, whether it warrants a new name or not, ML is concerned with being able to use synthetic data gengerated from the model for practical purposes, whereas statisticians usually refrain from doing that because it's problematic in many ways.

Prediction is a messy business, and when we say prediction what we mean is generating synthetic data and expecting it to be statistically similar to new real data, strictly speaking it should mean doing that within our range of observations, but because of our human experience and limitations we commonly think of predicting like something that will happen in the future -- I don't think there's anything preventing you from using it in that sense, but the less ambiguous term for it would be forecasting. 
Forecasting is even messier, it's different than prediction because it means predicting the future, statistically, and relies on the future resembling the past and the present; extrapolation is also a kind of prediction, one that is out of the range of the original model, so forecasting is a kind of extrapolation in time, but we can extrapolate in space, or in any variable in our model. For instance, we can fit a straight line in the xy plane wiht observations between 0 and 10, and then use the model to extrapolate what observations would look like between 15 and 20.

We can never know if the future will resemble the past, or if observations outside of a range behave the same way as within them (maybe a wholly different model is needed outside of these bounds for whatever reason); and even within some range, maybe the parameters are not fixed in time and next time we try to obserrve something we assume is stable we get very different results, so statisticians were always cautious in generalizing. Maybe that's the greatest difference from Machine Learning, that's the whole point of ML. But if the math is the same, why would ML be able to do something that SM is not? The answers is it's not, it must resort to other means and sacrifice some things that statisticians find important. Again, these are still statistical models and the techniques all essentially existed before, but the way they are used is qutie different from statistical modeling, ultimately causing a split in the fields that have a lot in common. 

+ **Video**: Basics of ML practice: training, developing, testing
The term "Machine Learning" was created shortly after "Artificial Intelligence", which is a broader term that doesn't necessarily involve data. Statistical AI is a related term, because it realies on data and statistical patterns to deliver some sort of "intelligence" (and we'll get there and discuss "intelligence"). The original idea was replacing explicit instructions with data and algorithms that can inform some sort of decision. The field is not brand new, and it was kind of in the backburner for a few decades, like AI, but it was just what was needed once there was enough compute and data available.

With a lot of data, however, some of the traditional statistical models fall short because they cannot describe all patterns, so you need larger and more flexible models, and that's another characteristic often associated to Machine Learning, and another one that is not exclusive to it, but again is more common in ML than SM. But simply increasing the size and complexity of models to be able to fit the data creates other complications, so the approaches must be sound to achieve good practical results, and ML is all about practical results, while SM is more about understanding real systems.

- Bias Variance Trade-offs
So how do we go about applying Machine Learning? Well, the same way we go about applying Statistical Modeling, we choose a model and infer parameters. Depending on the data and problem at hand, sometimes we use the same models, like linear regression and logistic regression, the latter is incredibly popular, and if you didn't know better and picked up a random ML book you'd think Machine Learnists invented it. The main difference up to this point is that you're most likely to hear statistical inference called "training" instead, or alternatively you may hear the model is "learning" the parameters or some variation of learn, but it means exactly the same as inference of parameter estiamtion.

As I mentioned, fitting a bigger model to a lot of data may cause problems, like inferring a pattern that is not actually there. The simple way of thinking of it is a model that only has one parameter, but you can explain the data with two or three parameters because the others are redundant, but if you try to actually interpret the additonal parameters you can'tm they are meaningless or spurious. I'm pulling this from long term memory now, but I think it was John Von Neumann that said "give me four parameters and I can fit an elephant, give me five and I'll make it wiggle its trunk", with the meaning that you have too many moving parts, if you have too many parameters you eventually start to fit noise instead of fitting signal, you start to have parts of your model trying to provide a deterministic structure to what should really be randomness.

There's a name for this, that I don't think it's a very intuitive one, but it's called the bias-variance trade-off. This is almost always illustraed with this graph of the "bias" decreasing as a function of "model complexity" and "variance" increaseing, such that the best model is somewhere in the middle where they balance each other out and there's some sort of minimum. The reason I find this misleading is because "model complexity" is not something objectively defined, it's not the same as the number of parameters, and you can also have different models with the same number of parameters or models with more parameter that could be considered less complex than one with more. PLus, the terms bias and variance aren't really trivially associated to their more common meaning in statistics, their intended meaning of decreasing bias is that the model gives a better fit to the data, and increasing variance actually means that new samples not used in training will have greater error because you're focusing too closely on the samples used for inference.

The classic example you'll see of this is fitting a polynomial of low order, maybe two or three, so a function like ax^3 + bx^2 + cx + d, and say we have ten or fifteen data points. You can always fit a straight line through the data, but in this case it would never be able to capture the correct trend, you'd need the additional powers. But we almost never know from the data alone what the model is, so on the other extreme we may decide to fit a polynomial of order five or ten, or fifteen, and a polynomial with order N will fit exactly without error any data with the same number of points, but in this case it will produce trends that don't exist in the right model. So instead of parroting bias-variance, I prerfer to give a slightly longer explanation for this, in one case thte model is too little, in another it's too much. In practice you'll probably never get asked "where are we in the bias-variance trade-off?", you just have to understand how this works and the situations you may be in if you choose a model that may be too simple or too complicated. 

What will happen in Machine Learning is computing these two quantitities and seeing their behavior, and if you understand the gist of it you'll know what you need to do about it. How do we acutally go about it after doing our inference?

- Training/Dev/Test splits

What is different as a matter of practice, is that instead of using the entire data set for inference, only a part of it is used initially, the fraction varies, but it's often 80-95% depending on the total amount of data, it can be 99% of the data set or more if it turns out that this 1%, half a percent or whatever is a still a plenty of data to compute the remaining tasks.

Whatever it is, this larger initial fraction is called the "training set", we saw that parameter estimation is called training in ML, so this is exactly what this seems to mean, the fraction of the data set used to estimate parameters, that's why we need the most data here. Another purely nomenclature difference you'll see is the term "loss function" replacing likelihood or sum of squared error, but it means the same thing; it's actually useful to some extent because sometimes it's neither so it's more general, but as before it can also mislead you into thinking it's something else and not something you already understand. If you know what The Likelihood or the Least Squares method is, it is something you already understand.

The remainder of the data is used for two related things. You may see data plits as Training+Testing, but that's limited because once you tested your data you can only say whether the model good or not and, if you have more than one model, which model performs better. So a better description of the split is Training+Validating+Testing. Validating, also called development or holdout set is used to compute the error of the model on "new" data -- of course it's not new data, but from the point of view of the model it doesn't matter if you had the data to begin with or if you just didn't make the data available to the inference algorithm, so here we're already in "prediction" territory, but what we actually want is to choose between models or maybe introduce other options that have better predictive performance.

This approach is again not a new thing from Machine Learning, it's related to statistical cross-valiation. If you've been looking into any literature you may see LOO-CV, which stands for Leave-One-Out Cross-Validation, that is performing inference with a single data point left out, if you do that leaving out each single data point you get a measure of error of a certain model. There's also LKO-CV, which is Leave-K-Out cross-valiation, that means you don't leave one, but whatever number: 2,3,4,10,K. you could even compute every possible combination of points left out, the thing is inference is often expensive, so you don't want to have to redo all calculations. There are alternatives that allow you to approximate LOO-CV without having to rerun inference, but that's beyond the scope of our course. 

So you can use LOO-CV or LKO-CV with any data points. In Machine Leagning, validation is done using this particular "devevelopment" or "validation"  data set which I'll also refer to as dev for short sometimes. These are observations there were not used for training. This measures or error will allow you to pick the best performing model.

The test set is the final holdout, it's used to get a final measure of performance that is unbiased, in the sense that, if you use the valiation set to choose the best model, it obviously performs best on this dev set. Test sets give you the final word. 

In summary, if everything works well, we have a lof of data to train the models, we choose the best model using a validation set, and the test set confirms that the accuracy of that model on "new", again quote-unquote data is very high and we have a great model for our application. This all hinges on the three data sets being comparable; it's impossible for a model to perform well on the dev or test set if it's qualitatively different from the training set. That is sometimes called distributional shift or similar terms, because it means we had a certain distribution in the test data and it somehow shifted in the other data sets. That's kind of a buzzword, because it's an obvious corllary from using data that can differ, and there's almost always no real shift per se. There are techniques we're not going much into to make sure this doesn't happen, one obvious one is to randomly choose the data that goes into each data set -- if you hve temporal data that for some reason changed at some point in time, and just split the early 90%, 5+5%, maybe you get differences between sets, but if you randomly sample data points from the whole set and place them in each data set that is less likely to happen. We're not going to devel into this too much, but hopefully you get the idea.

- Regularization

Another common practice item in Machine Learning that comes from statistics, but you can't miss in ML is regularization. To me it's not a very self-explanatory term, but what it means is somehow simplifying the model, so the consequence of it is reducing overfit. The classic examples are the L1 and L2 penalties, which are the distinguishing features of LASSO and Ridge regression. I'm going to use them as an example for regularization in general.

In both cases what is done is we are explicitly adding a penalty term to the loss function, without more information this alone will tell you that the parameters will not have the same value as if you didn't have it -- it would be mathematically impossible. This penalty term has a hyperparameter, which is a buzzwordy term for saying it's not a parameter in the model, but a parameter of the inference method or algorithm itself, called lambda (sometimes alpha) that multiplies either the absolute value of the parameters (in LASSO) or their squares (in Ridge).

As I was developing the notes for this video I realized regularization is not intuitive at all, and that's why people often simply go straight for explaining the actual math, and try to relate to what is the intention with some conceptual context, but it's often lost in translation, so let me back up a little bit here and try to explain it conceptually first.


### ALTERNATIVE BOILER-PLATE INTRO
Regularization is a set of techniques used in statistical modeling and machine‑learning to prevent over‑fitting, which occurs when a model captures noise in the training data rather than the underlying signal. By adding a penalty term to the loss function—such as the L2 norm (ridge) or L1 norm (lasso)—the optimizer is forced to trade off fitting the data perfectly against keeping the model parameters small or sparse. This extra constraint discourages overly complex models, leading to better generalization on unseen data.

The presence of a penalty term directly induces shrinkage, meaning that the estimated coefficients are pulled toward zero compared to an unregularized fit. In ridge regression, every coefficient is reduced proportionally to its size, while lasso can drive some coefficients exactly to zero, performing variable selection. Shrinkage reduces variance at the cost of a modest increase in bias, which often yields a lower overall mean‑squared error—a classic bias‑variance trade‑off.
###

- Least Absolute Shrinkage and Selection Operator (LASSO)
- Ridge






