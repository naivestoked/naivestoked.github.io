Module 7. Artificial Intelligence
Artificial Intelligence is a senior person now, it's as old as my parents, but it was dragged around so much in the past few years that it becase almost a meaningless umbrella term for almost anything, whether it involves statistics, machine learning or just has a temperature sensor and a breaker or something. So we're going to try to keep it real and within the constraints of what is objective and useful.

Module-level objectives:
Following our Machine Learning module we're going to ground our definitions in Statistical Models and discuss the SM/ML roots of what is being called AI in the wild (and which deserves our time and attention), particularly complex applications like Large Language Models and Transformers-based applications, popularly called GenAI, but also other applications with names like Predictive AI, World Models, etc.

1. Establish a clear connection between SM/ML methods and real-world AI applications
2. Recognize the limitations in Generative AI
3. Describe the Underlying mathematics and statistics of AI

VII. Artificial Intelligence
    + **Video**: Generative AI (a.k.a Machine Learning that outputs things through a different generative process, but that look like humans did it)

    I may have used the term "Generative Model" a couple of times, maybe more, over the previous presentations, and that may have confused some of you since we're now so used to hear about Generative AI, or GenAI, and what I said most likely had nothing to do with that. That's because it's unfortunately common practice to reinvent the terminology wheel whenever a new field is created, or whenever people think a new field has been created, or even when people know it nothing new has been created but they feel like they need to pretend they did and plant a flag to show it's theirs.

Generative AI is one of those terms, I couldn't even find the origin of it, but for us looking at statistics it's particularly confusing because "Generative Models" means something quite different here, it means a process that has generated our observation. So in an epidemic, the generative model is the transmission process itself, it's the model that generated the disease cases -- we may not know all the relevant details: the heterogeneity in susceptibility of individuals, the mobility and contact patterns of each of them, the precise replication dynamics of the virus that may affect the transmissibility, the precise time distribution of incubation and recovery periods, but we do know a lot about the most important essential processes for transmission (for instance, those encoded in the SIR model simulation, which makes some assumptions about those things, even if they are approximations). In other cases, like RNA-Seq data, we know very little to nothing about the generative process, because transcription of RNA can involve the entire organism, feedback loops, metabolism, environmental interactions, etc, so we may be able to formulate similar mechanistic models for some systems like the circadian clock, or simplify some processes like the initial proliferation of a virus upon infection (before the adaptive immune system kicks in with T-cells, B-cells and then hugely complex interactions taht we probably cannot accurately formalize into something really workable computationally).

So anyway, GenAI is kind of the opposite of this, it has nothing to do with the actual process that normally generates the kind of things we see it output -- text, images, video, these have to be crafted and recorded by humans in conginitively intense processes -- instead it just generates stuff through a completely different method, like the neural networks we saw that just crunch a bunch of vector and matric processes in parallel and gives an output of whatever we archtected them to do, and it gets its name from that.

What's special in GenAI? What's special is that it works well, and it works well basically because of scale, at some point the data sets were enough to generate convincing quality output. That's not to say that you could just throw data at it and some point it would work. As we also commented, the kind fo recurrent neural networks we saw were not incredible in performance, and the transformers architecture allowe a qualititative improvement in performance. Neverthless, it's also not to say that it alone made GenAI. The most known paper on The Multi-headed attention mechanism that is the basis of transformers, with the title "Attention is all you need" by Google researchers is from 2017, and the Attention mechanism predates that, so it took at least 5 years when all the pieces were in place for us to see a convincing Large Language Model in the wild.

And here credit where credit's due, other companies might have had similarly perfoming chatbots even before, but it was OpenAI who decided to put it out there to the public despite it being far from optimal, and plagued by problems that to this day haven't been properly solved like the quote-unquote "hallucionations" (important parenthesis, in case I haven't drilled on this, a better term for this is "bullshitting" in the philosophical sense proposed by Harry G. Frankfurt). Maybe now we look back at the first chatGPT model and think it's garbage compare to the current so-called "frontier models", but it was really impressive then, the thing is it was based on GPT-3.5, so we can be basically sure that 1 throuhg 3 were underwhelming in that sense.

(At this day and age there's enough lore on chatbots that you may know this already, but in case you don't, GPT stands for Generative Pretrained Transformer, so Generative was already in the jargon there, even when it wasn't generating much -- FYI pretained means it's parameters were estimated beforehand, so we now konw that this model went through the entire Machine Learning development cycle, and transformers is one of the main characteristics of this kind of Neural Network architecture.)

Nobody really knows why it actually started working well at that point, but some of the becnhmarks show for some architecture that had passable performance that just increasing the number of parameters at some point say a qualitative jump in performance as well as a increase in the rate of improvement with parameter number and training data size. It wasn't just magic-like either, though, some post-trainintg like fine-tuning and Reinforcement Learning from Human Feedback (RLHF) was heavily used to make LLMs the products we know, this realies on massive amounts of human curation work, and that's outside the territory of statistical modeling as we discussed it.

The details are not fully konwn because a lot of this is proprietary, closed information about each product, but I belive there's a lot of trial and error invovled until getting to the desired level of performance the companies putting these out need.

So, in summary, what can we say we know about Generative AI? I cannot honestly say I explained all the details you need to implement your own LLM from scratch, but I'm convinced that if you really want and have or develop the mathematical tools you could spend some time (and maybe it's quite a bit of time) to do it. At the very least I'm sure you could pick any given component and understand what is doing, what it represents, and what assumptions and choices were made to put it there. More importanly, with the math & statistics fundamentals, the statistical modeling core and the machine learning workflow you should definitely be able to describe what it takes to get any model, GenAI or otherwise up an running from beginning to end.

Finally, this is just a pet peeve of mine on wrong usage of proper terminology. You may see people, especially people who know nothing about statistics calling the process of prompting and getting an output from a model "inference". I couldn't find any rhyme or reason for this either, and as far as I'm concerned it's totally wrong, there's no inference going on there as we've seen, the model is fixed at that point with all its parameters, what we are doing here is equivalent from giving an input like the initial conditions of an epidemic model and getting some epidemic curves. So you can call it generating output, simulating, even prediction, but I see no way this can be called inference in any reasonable way. So if you get nothing else from this course, at least take this and now that you won't be crushing the souls of poor statisticians trying to maintain a tiny bit of sanity in the middle of this all. But seriously, you can see why it would be confusing calling this inference, and after this entire course you know much better than taking the very statistical underpinnings of these models and the statements about them acritically, at face value, as something only tech wizzes could understand or a magic only tech titans can manipulate.

Is all Statistical Modeling, from beginnign to end. 

    + **Video**: AI vs AGI: what is all the hype about?

# (Optional) AI vs AGI: what is all the hype about?

So now that we did Hype 101 in the previous topic, let's jump straight to Hype 504: Artificial General Intelligence, a.k.a. Artifiicial Superintelligence, the Singularity, AGI, ASI, and so many concepts that seem like they came straight out of science fiction. Here I have to make a disclaimer that this topic is not going to be extremely scientific because there's little science to support many of these notions; nevertheless, I need to address the trillion-paramete elephant in the room, so whatever I say here will not change the math and science we discussed so far, but it will venture into: hardcore speculation, science-fiction, wishful thinking of some of the intersted parties, and maybe outright make believe proferred by people motivated by any other reasons rather than hard, scientific and mathematical facts.

First things first, I'll use the term AGI because it's the most commonly used, but I'll clarify that there's really no formal difference between any of them, because none of them are based on anything concrete. In theory, and this is already a theory based on a non-existent entity, an aritificial general intelligence would be capable of everything human intelligence is, but would be able to access all of human knowledge, just like LLMs already can (in the sense that they actually read, recorded, or encoded this knowledge they can access instantly), so an AGI could develo knowledge at a rate impossible to humans, and would potentially not have other human constraints, like developing one thought at a time. Therefore, it would be capable of solving any problem nearly instantly,a dn would be a supperintelligence and superbeing like the Supreme AI from the Kree in the Capitain Marvel movie, if you're into Marvel movies -- I'm really not, but that one got me with the 90s nostalgia style, and at least I got something from it like a depiction of what would a society be if it was ruled by a Supreme Superintelligence.

So AGI, Superintelligence, all the same, Singularity refers to a point where, if those exist, humans would have no control over their own future, if they even had one. We would probably be disposable. Again, we're in sci-fi speculation mode here, so what would happen next if these companies actually achieved anything like AGI, could it be controlled to solve all of humanities problems and we could live in a proper utopia? Wihtout getting political about the way humans organize, I really doubt it would go down that smoothly. But the "good news" is that there's really no sign that there's anything in the near, or faraway future that looks anything like this.

So back to reality, and to the origins of AI, when Aim was created as a discrete concept, there was no concept of AI vs AGI, it was all AI, this was supposed to be general, but what, was achieved through the different approaches of symbolic AI, Statisical AI etc is what we're seing. We also saw that the most quote-unquote "intelligent" systems we have are based on raw scale in data and compute, nothing like any kind of intelligence we know, and yet, they are incapable of anything beyond generating output from a given input. How could they? They are statisical models.

So when we speak of a superbeing, there's no being, they are a coded statistical model inside a computer, they cannot do what a human can. They can do other tasks very well, like quickly generate a text on almost any topic (the acutal quality in many respects is also a matter of debate, but they have many limitations even in the narrow tasks they are given in many cases).

That is not to say that AGI is technically impossible, I cannot prove that, and I'm not sure anyone currently can, in the positive or the negative, but you may hear that we're already on our way to AGI by improving current LLMs. That was wishful thinking even before we had evidence on the contrary. That sort of dicontinuity that seems to have appeared at some point around GPT-3 is what some expected to see again if they just scaled the data and models up and up and up  -- you've seen the claims about some of the latest models, some companies are more megalomaniac in that sense then others, I'm not going to comment further on individual companies or leaders, but I'm sure you've seen this plenty -- OpenAI published a paper touting scaling quote-unquote "laws" for LLMs, trying to justify the monumental resource investiments we've been seeing. 

Not only did this new quantum jump not happen, the opposite seems to be happening, and that might have been more predictable, we're seeing diminishing returns on the scaling up; over three years in there's a lot of debate about hype versus concrete gains, people who like it more than others, cost-benefit, climate, and there's a concrete reality that may be perceived differently by different people, but I don't think anyone can honestly claim that there's any real sign of AGI. Some companies are already moderating their expecations and their discourse, saying things like "AGI will emerge from a distributed network of AI agents" -- I think that makes even less sense than the claim of achieving superintelligence, but since it's less tangible it's harder to say they were wrong, so they can walk back their earlier claims without greater consequence.

The scenario is compatible with what Sayash Kapoor and Arvind Narayanan from Princeton have described as AI as Normal Technology. I'm not going to describe it here because they have plenty of high-quality reading and media about the topic, so I recommend looking into their blog and other work, because I think it's a very grounded perspective on AI. 
What we can say in general (no pun intended) is that AI, a.k.a. Machine Learning, a.k.a. Statistical Modeling will be able to solve the problems they are well suited to solve, and we should understand their statistical underpinnings to know what these problems are and what they are not -- you may see the term "Narrow AI" to constrast with AGI, but normally I don't think it's a useful term. There's no upper limit to hype: superintelligence, ultraintelligence, megaintelligence, petaintelligence...

As we've seen, even AI/ML can be misleading in some sense, so my final message in this kind of ranty take on the putative connection between AI and what we call intelligence in humans is to really look at the fundamentals.

    + Limitations of the Blind use of AI: Vibe coding, AI Slop, etc

    + **Video**: SM/ML/AI: what is the (mathematical and practical) difference, and does it matter? Some reflections and conclusions

# SM/ML/AI: what is the (mathematical and practical) difference, and does it matter? Some reflections and conclusions

So finally, very briefly just to wrap up our AI/ML/SM parallels after hyaving discussed everything from dot products to Capitan Marvel...

I like to summarize it like this: Artificial Intelligence is just Machine Learning, is just Statistical Modeling, which in the context of automation is just Algorithms where implicit instructions, or what we at some point hoped would be intelligence is replaced by data and code that can give us some output we can use in practice.

As I mentioned, there are indeed differences in the fields, in some nomenclature, objectives, practices, but it does boil down to the same core of concepts. To me it matters mostly in that I do nto want to lose touch with the fundamentals that make all of this possible. I'm a scientist, so I'm interested in knowledge and how we can lose this to learn interesting things, and I'm also interested in defending the scientific process against automating things that are not formally not automatable (although I'm in favor of automating everything else). To others, it maybe other things that matter, that's the reason why the Machine Learning community is different from the Statistics community, and people beyond academia may want to use all of these in whole different ways.

For some, maybe the distinction is not important at all, or they don't care. But I think even for an enterpreneur who just wants to applied whatever tech to create a new and amazing product, it's important to understand their potential and limitations. As researchers, I expect you to take this with you from this course: the understanding of how this connects from the most basic levels to the application level so you can take it in any direction you like.

    + Group Discussion: What is AI good for?
    + Essay assignment: Write three interesting features about an available AI application as bullet points, elaborate on these bullet points and explain how ML underlies them live or via recorded video, .